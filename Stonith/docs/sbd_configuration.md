## Prerequisite

  - Create shared based disk from volume pools up to 100 MB.
  - Example: /dev/disk/by-id/dm-name-mpathi

## Stonith

1. Install sbd (All Nodes)
  ```bash
      yum install -y sbd fence-agents-all

      version:
          Installing:
          fence-agents-all           x86_64 4.2.1-30.el7_8.1
          sbd                        x86_64 1.4.0-15.el7
  ```

2. Assume that shared volume `/dev/disk/by-id/dm-name-mpathi` is available on LUN other than 0.

3. Create Physical volume (All Nodes)
  - Create physical volume for SBD.
  ```bash
    pvcreate /dev/disk/by-id/dm-name-mpathi
  ```

4. Create sbd device on one node (/dev/disk/by-id/dm-name-mpathi)
  ```
    sbd -d /dev/disk/by-id/dm-name-mpathi -4 30 -1 20 create
  ```
  - In above `sbd` command, -1 is watchdog_timeout and -4 msgwait_timeout
  - If using multipath then for /etc/multipath.conf the value of max_polling_interval must be less than watchdog_timeout.
  - msgwait_timeout must be grater than watchdog_timeout.
  - Check volume status from sbd.
  ```bash
    sbd -d /dev/disk/by-id/dm-name-mpathi dump
  ```

5. Load softdog module for watchdog (All Nodes)
  ```bash
    modprobe softdog
    echo softdog > /etc/modules-load.d/watchdog.conf
  ```

6. Edit sbd conf /etc/sysconfig/sbd (Both)
  ```conf
    # This file has been generated by pcs.
    SBD_DEVICE="/dev/disk/by-id/dm-name-mpathi"
    SBD_OPTS="-n srvnode-1" # Enter pacemaker node name
    SBD_TIMEOUT_ACTION=flush,off
  ```

7. Enable sbd (All Nodes)
  ```bash
    systemctl enable sbd
  ```

8. Restart cluster to start sbd (Primary Node)
  ```bash
    pcs cluster stop --all
    pcs cluster start --all
  ```

9. Set Cluster Property (Primary Node)
  - For two node cluster stonith-watchdog-timeout must be 0.
  - stonith-timeout is time to detect node and do fencing for node. It must be grater than msgwait_timeout. Refer step (4) for msgwait_timeout.
  ```bash
    pcs property set stonith-watchdog-timeout=0
    pcs property set stonith-timeout: 40s
    pcs property set stonith-action=off
  ```

10. Create stonith resource (Primary Node)
  - power_timeout must be grater than stonith-timeout and msgwait_timeout.
  ```bash
    # Single Resource (For self fencing)
    pcs stonith create sbd fence_sbd devices=/dev/disk/by-id/dm-name-mpathi pcmk_host_check=static-list pcmk_host_list="srvnode-1 srvnode-2" power_timeout=50 delay=5 meta failure-timeout=15s

    # Two Resource (For opposite fencing)
    pcs stonith create sbd-c1 fence_sbd devices=/dev/disk/by-id/dm-name-mpathi pcmk_host_check=static-list pcmk_host_list=srvnode-1 power_timeout=50 delay=5 meta failure-timeout=15s

    pcs stonith create sbd-c2 fence_sbd devices=/dev/disk/by-id/dm-name-mpathi pcmk_host_check=static-list pcmk_host_list=srvnode-2 power_timeout=50 meta failure-timeout=15s

    # For opposite node fencing
    pcs constraint location sbd-c1 avoids srvnode-1
    pcs constraint location sbd-c2 avoids srvnode-2

    # Note: we cannot use two resource for self fencing. Use single resource for self fencing.
     --------------------------------------------------------------------------------
    | Error: for two resource                                                        |
    | notice: can_fence_host_with_device:     sbd-c1 can not fence (off) srvnode-2:  |
    | static-list                                                                    |
    ---------------------------------------------------------------------------------
  ```

11. Set stonith level (Primary Node)
  - Consider we already have two stonith resource stonith-c1 and stonith-c2 of ipmi resource.
  ```bash
    pcs stonith level add 1 srvnode-1 sbd-c1
    pcs stonith level add 2 srvnode-1 stonith-c1
    pcs stonith level add 1 srvnode-2 sbd-c2
    pcs stonith level add 2 srvnode-2 stonith-c2

    # For Single resource
    pcs stonith level add 1 srvnode-1 sbd
    pcs stonith level add 1 srvnode-2 sbd
  ```

## Check Stonith Configuration

1. stonith full view
  ```
    pcs stonith show --full
      ```Output
        Resource: stonith-c1 (class=stonith type=fence_ipmilan)
          Attributes: auth=PASSWORD delay=5 ipaddr=<srvnode-1_bmc_ip> lanplus=true login=<bmc_user> passwd=<bmc_pass> pcmk_host_check=static-list pcmk_host_list=srvnode-1 power_timeout=40
          Operations: monitor interval=10s (stonith-c1-monitor-interval-10s)
        Resource: stonith-c2 (class=stonith type=fence_ipmilan)
          Attributes: auth=PASSWORD ipaddr=<srvnode-1_bmc_ip> lanplus=true login=<bmc_user> passwd=<bmc_pass> pcmk_host_check=static-list pcmk_host_list=srvnode-2 power_timeout=40
          Operations: monitor interval=60s (stonith-c2-monitor-interval-60s)
        Resource: sbd-c1 (class=stonith type=fence_sbd)
          Attributes: delay=5 devices=/dev/disk/by-id/dm-name-mpathi pcmk_host_check=static-list pcmk_host_list=srvnode-1 power_timeout=50
          Operations: monitor interval=60s (sbd-c1-monitor-interval-60s)
        Resource: sbd-c2 (class=stonith type=fence_sbd)
          Attributes: devices=/dev/disk/by-id/dm-name-mpathi pcmk_host_check=static-list pcmk_host_list=srvnode-2 power_timeout=50
          Operations: monitor interval=60s (sbd-c2-monitor-interval-60s)
        Target: srvnode-1
          Level 1 - sbd-c1
          Level 2 - stonith-c1
        Target: srvnode-2
          Level 1 - sbd-c2
          Level 2 - stonith-c2
      ```
  ```

2. Check
  ```bash
    fence_sbd --devices=/dev/disk/by-id/dm-name-mpathi -n srvnode-1 -o status
      ```Output
        Status: ON
      ```

    fence_sbd --devices=/dev/disk/by-id/dm-name-mpathi -n srvnode-2 -o status
      ```Output
        Status: ON
      ```

    pcs stonith describe fence_sbd
  ```

## SBD Commands

1. Check
  ```bash
    # Dump sbd metadata
    sbd -d /dev/disk/by-id/dm-name-mpathi dump
      ```Output
        ==Dumping header on disk /dev/disk/by-id/dm-name-mpathi
        Header version     : 2.1
        UUID               : f61fe4dc-558d-4f3d-aefb-57e6d3dd5fb9
        Number of slots    : 255
        Sector size        : 512
        Timeout (watchdog) : 20
        Timeout (allocate) : 2
        Timeout (loop)     : 1
        Timeout (msgwait)  : 30
        ==Header on disk /dev/disk/by-id/dm-name-mpathi is dumped
      ```

    # List watchdog nodes
    sbd -d /dev/disk/by-id/dm-name-mpathi list
      ```Output
        0       srvnode-1       clear
        1       srvnode-2       clear
      ```

    # Off node
    /sbin/sbd -d /dev/sdb message srvnode-2 off

    # ON node
    /sbin/sbd -d /dev/sdb message srvnode-2 clear

    # Reboot node
    /sbin/sbd -d /dev/sdb message srvnode-2 reset

    # Test message
    /sbin/sbd -d /dev/sdb message srvnode-2 test
  ```

## Result:
  - Level 1 sbd and 2 ipmi
    - Both Running ipmi, sbd and fencing happen
      - sbd shutdown faulty node node
    - ipmi failed but sbd running
      - sbd shutdown faulty node node
    - sbd failed but ipmi running
      - failed node will poweroff by ipmi
    - IF sbd configure to fence other node
      - If resource fail to stop on single node then IEM may reach to csm but node will not get fence.
    - IF sbd configure to fence self node
      - If resource fail to stop on single node then single node will get off.

## Drawback
  - Replace node may need to restart cluster to enable sbd
